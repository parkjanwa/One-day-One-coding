{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch기초.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ASVRf_OFEr6"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IwCrOEVBL5e"
      },
      "source": [
        "#  자동 미분 흐름 다시보기1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu79TL-kBKww",
        "outputId": "09392854-285c-4dde-c185-62a24cab068c"
      },
      "source": [
        "a = torch.ones(2, 2)\n",
        "print(a)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rng2Qfn1BgeP",
        "outputId": "b1b7ed7f-d4c5-4feb-9faa-4f3cb848d012"
      },
      "source": [
        " a = torch.ones(2, 2, requires_grad=True)\n",
        " print(a)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QIS4YYDBmRV",
        "outputId": "5334e401-6097-4698-d78c-515ba023fa04"
      },
      "source": [
        "print(\"a.data:\", a)\n",
        "print(\"a.grad_fn\", a.grad_fn)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a.data: tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "a.grad_fn None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izf2CaF-Bx-u",
        "outputId": "3f721d9f-ccf3-4472-a8ee-43ff1c8cecc1"
      },
      "source": [
        "b = a + 2\n",
        "print(b)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN862vo3CP04",
        "outputId": "5645e62b-281d-4c08-ad24-3fe00f709cac"
      },
      "source": [
        "c = b ** 2\n",
        "print(c)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[9., 9.],\n",
            "        [9., 9.]], grad_fn=<PowBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilDyFYjFCTb8",
        "outputId": "177ab9dd-3a9f-4fb8-dece-f5fcded2ad1a"
      },
      "source": [
        "out = c.sum()\n",
        "print(out)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(36., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24xKhWPdCY_C",
        "outputId": "16906714-9ab3-4c1a-8450-2f61d97c93b8"
      },
      "source": [
        "print(out)\n",
        "out.backward()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(36., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bBhdocZDQd6"
      },
      "source": [
        "### 출력해보면서 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cAbdPPhCi6-",
        "outputId": "22afc70d-ced0-42ad-cb55-9e9d2fcdd86a"
      },
      "source": [
        "print(\"a.data:\", a)\n",
        "print(\"a.grad: \", a.grad)\n",
        "print(\"a.grad_fn\", a.grad_fn)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a.data: tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "a.grad:  tensor([[6., 6.],\n",
            "        [6., 6.]])\n",
            "a.grad_fn None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4RTW_aHCwPU",
        "outputId": "59120329-b29f-4027-f365-3bb777a3d930"
      },
      "source": [
        "print(\"b.data:\", b)\n",
        "print(\"b.grad: \", b.grad)\n",
        "print(\"b.grad_fn\", b.grad_fn)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b.data: tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "b.grad:  None\n",
            "b.grad_fn <AddBackward0 object at 0x7f5d7b3580d0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpkxaxfAC7ri",
        "outputId": "5f98a56d-73b4-43c6-aeeb-8524323b619a"
      },
      "source": [
        "print(\"c.data:\", c)\n",
        "print(\"c.grad: \", c.grad)\n",
        "print(\"c.grad_fn\", c.grad_fn)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c.data: tensor([[9., 9.],\n",
            "        [9., 9.]], grad_fn=<PowBackward0>)\n",
            "c.grad:  None\n",
            "c.grad_fn <PowBackward0 object at 0x7f5d7b3c3410>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8zYHOLLC-uj",
        "outputId": "5ef9bc04-0858-4199-d08f-bac461a3b582"
      },
      "source": [
        "print(\"out.data:\", out)\n",
        "print(\"out.grad: \", out.grad)\n",
        "print(\"out.grad_fn\", out.grad_fn)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out.data: tensor(36., grad_fn=<SumBackward0>)\n",
            "out.grad:  None\n",
            "out.grad_fn <SumBackward0 object at 0x7f5d7b366150>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGMEh0X3E4eg"
      },
      "source": [
        "# 자동 미분 흐름 다시보기2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCuJrNuJDNY8",
        "outputId": "90b6bd49-0eda-4e5e-cf7b-127778a8aea9"
      },
      "source": [
        "x = torch.ones(3, requires_grad=True)\n",
        "y = (x ** 2)\n",
        "z = y ** 2 + x\n",
        "out = z.sum()\n",
        "print(out)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89uCvI1aFCrb"
      },
      "source": [
        "grad = torch.Tensor([0.1, 1, 100])\n",
        "z.backward(grad)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndhm_cGTFSTO",
        "outputId": "53103a5f-da52-436e-9c27-66a7f28dd9da"
      },
      "source": [
        "print(\"x.data:\", x)\n",
        "print(\"x.grad: \", x.grad)\n",
        "print(\"x.grad_fn\", x.grad_fn)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x.data: tensor([1., 1., 1.], requires_grad=True)\n",
            "x.grad:  tensor([  0.5000,   5.0000, 500.0000])\n",
            "x.grad_fn None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1BYBEBiFrl2",
        "outputId": "0ab9f81d-6acc-4a99-88f3-9560d16989fc"
      },
      "source": [
        "print(\"y.data:\", y)\n",
        "print(\"y.grad: \", y.grad)\n",
        "print(\"y.grad_fn\", y.grad_fn)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y.data: tensor([1., 1., 1.], grad_fn=<PowBackward0>)\n",
            "y.grad:  None\n",
            "y.grad_fn <PowBackward0 object at 0x7f5d7b34df10>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJRt1zXIFrn2",
        "outputId": "b714dc79-248a-4cb7-caf1-a88bb1ccd6b6"
      },
      "source": [
        "print(\"z.data:\", z)\n",
        "print(\"z.grad: \", z.grad)\n",
        "print(\"z.grad_fn\", z.grad_fn)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z.data: tensor([2., 2., 2.], grad_fn=<AddBackward0>)\n",
            "z.grad:  None\n",
            "z.grad_fn <AddBackward0 object at 0x7f5d79402a10>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}